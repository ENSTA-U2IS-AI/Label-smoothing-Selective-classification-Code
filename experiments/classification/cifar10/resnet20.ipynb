{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary Material - Towards Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It\n",
    "\n",
    "This is the code was included in the supplementary material of our ICLR submission **487**. Namely, the code:\n",
    "- **trains two ResNet-20**, one with label smoothing and one without, on CIFAR-10\n",
    "- **plots the Risk-Coverage (RC) curves** of the two models and computes the areas under these curves\n",
    "- **shows the effect of logit normalization** on the Risk-Coverage curves\n",
    "\n",
    "You will need to be on a machine with at least a small GPU to run the code properly. The whole notebook will run in approximately **15 minutes** with a consumer-grade GPU including training. If you have no time, you can set TRAIN_MODELS to False and use the networks that we trained in advance to compute the RC curves. This code was designed for linux/MacOS, but small changes (e.g. in the requirements to install the GPU version of PyTorch) should make it work on Windows.\n",
    "\n",
    "The rest of the codes (ImageNet & segmentation trainings and models) will be made available online after the anonimity period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the virtual environment\n",
    "\n",
    "We start by creating the virtual environment and downloading the necessary packages (torch, torchvision, & ipykernel). If you already have an environment (conda or venv) including PyTorch and Torchvision, you can skip these steps. Otherwise, create a venv and install the packages with:\n",
    "\n",
    "```bash\n",
    "python3 -m venv .reproduce_env\n",
    "source .reproduce_env/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "The code was tested with Python 3.10. Please tell us if you encounter any issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, select .reproduce_env to run your notebook. Now, you can just run all the notebook to reproduce the results. You may delete the environment with `rm -rf .reproduce_env` when you are done to save disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can choose whether to train the models or used pre-trained ones, using the TRAIN_MODELS boolean variable\n",
    "TRAIN_MODELS = True\n",
    "num_epochs = 75\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the models\n",
    "\n",
    "We start by defining the ResNet-20 architecture and the training loop. We will train new models depending on the value of the TRAIN_MODELS variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "from typing import Literal\n",
    "\n",
    "from torch import Tensor, nn\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "\n",
    "class _BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_planes: int,\n",
    "        planes: int,\n",
    "        stride: int,\n",
    "        dropout_rate: float,\n",
    "        groups: int,\n",
    "        activation_fn: Callable,\n",
    "        normalization_layer: nn.Module,\n",
    "        conv_bias: bool,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes,\n",
    "            planes,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            groups=groups,\n",
    "            bias=conv_bias,\n",
    "        )\n",
    "        self.bn1 = normalization_layer(planes)\n",
    "\n",
    "        # As in timm\n",
    "        self.dropout = nn.Dropout2d(p=dropout_rate)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            planes,\n",
    "            planes,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            groups=groups,\n",
    "            bias=conv_bias,\n",
    "        )\n",
    "        self.bn2 = normalization_layer(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_planes,\n",
    "                    self.expansion * planes,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    groups=groups,\n",
    "                    bias=conv_bias,\n",
    "                ),\n",
    "                normalization_layer(self.expansion * planes),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        out = self.activation_fn(self.dropout(self.bn1(self.conv1(x))))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return self.activation_fn(out)\n",
    "\n",
    "\n",
    "class _ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block,\n",
    "        num_blocks,\n",
    "        in_channels: int,\n",
    "        num_classes: int,\n",
    "        conv_bias: bool,\n",
    "        dropout_rate: float,\n",
    "        groups: int,\n",
    "        style: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n",
    "        in_planes: int = 64,\n",
    "        activation_fn: Callable = relu,\n",
    "        normalization_layer: nn.Module = nn.BatchNorm2d,\n",
    "    ) -> None:\n",
    "        \"\"\"ResNet from `Deep Residual Learning for Image Recognition`.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_planes = in_planes\n",
    "        block_planes = in_planes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "        if style == \"imagenet\":\n",
    "            self.conv1 = nn.Conv2d(\n",
    "                in_channels,\n",
    "                block_planes,\n",
    "                kernel_size=7,\n",
    "                stride=2,\n",
    "                padding=3,\n",
    "                groups=1,  # No groups in the first layer\n",
    "                bias=conv_bias,\n",
    "            )\n",
    "        elif style == \"cifar\":\n",
    "            self.conv1 = nn.Conv2d(\n",
    "                in_channels,\n",
    "                block_planes,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                groups=1,  # No groups in the first layer\n",
    "                bias=conv_bias,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown style. Got {style}.\")\n",
    "\n",
    "        self.bn1 = normalization_layer(block_planes)\n",
    "\n",
    "        if style == \"imagenet\":\n",
    "            self.optional_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        else:\n",
    "            self.optional_pool = nn.Identity()\n",
    "\n",
    "        self.layer1 = self._make_layer(\n",
    "            block,\n",
    "            block_planes,\n",
    "            num_blocks[0],\n",
    "            stride=1,\n",
    "            dropout_rate=dropout_rate,\n",
    "            groups=groups,\n",
    "            activation_fn=activation_fn,\n",
    "            normalization_layer=normalization_layer,\n",
    "            conv_bias=conv_bias,\n",
    "        )\n",
    "        self.layer2 = self._make_layer(\n",
    "            block,\n",
    "            block_planes * 2,\n",
    "            num_blocks[1],\n",
    "            stride=2,\n",
    "            dropout_rate=dropout_rate,\n",
    "            groups=groups,\n",
    "            activation_fn=activation_fn,\n",
    "            normalization_layer=normalization_layer,\n",
    "            conv_bias=conv_bias,\n",
    "        )\n",
    "        self.layer3 = self._make_layer(\n",
    "            block,\n",
    "            block_planes * 4,\n",
    "            num_blocks[2],\n",
    "            stride=2,\n",
    "            dropout_rate=dropout_rate,\n",
    "            groups=groups,\n",
    "            activation_fn=activation_fn,\n",
    "            normalization_layer=normalization_layer,\n",
    "            conv_bias=conv_bias,\n",
    "        )\n",
    "        if len(num_blocks) == 4:\n",
    "            self.layer4 = self._make_layer(\n",
    "                block,\n",
    "                block_planes * 8,\n",
    "                num_blocks[3],\n",
    "                stride=2,\n",
    "                dropout_rate=dropout_rate,\n",
    "                groups=groups,\n",
    "                activation_fn=activation_fn,\n",
    "                normalization_layer=normalization_layer,\n",
    "                conv_bias=conv_bias,\n",
    "            )\n",
    "            linear_multiplier = 8\n",
    "        else:\n",
    "            self.layer4 = nn.Identity()\n",
    "            linear_multiplier = 4\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.flatten = nn.Flatten(1)\n",
    "\n",
    "        self.linear = nn.Linear(\n",
    "            block_planes * linear_multiplier * block.expansion,\n",
    "            num_classes,\n",
    "        )\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block,\n",
    "        planes: int,\n",
    "        num_blocks: int,\n",
    "        stride: int,\n",
    "        dropout_rate: float,\n",
    "        groups: int,\n",
    "        activation_fn: Callable,\n",
    "        normalization_layer: nn.Module,\n",
    "        conv_bias: bool,\n",
    "    ) -> nn.Module:\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(\n",
    "                block(\n",
    "                    in_planes=self.in_planes,\n",
    "                    planes=planes,\n",
    "                    stride=stride,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    groups=groups,\n",
    "                    activation_fn=activation_fn,\n",
    "                    normalization_layer=normalization_layer,\n",
    "                    conv_bias=conv_bias,\n",
    "                )\n",
    "            )\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def feats_forward(self, x: Tensor) -> Tensor:\n",
    "        out = self.activation_fn(self.bn1(self.conv1(x)))\n",
    "        out = self.optional_pool(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.pool(out)\n",
    "        return self.dropout(self.flatten(out))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.linear(self.feats_forward(x))\n",
    "\n",
    "\n",
    "def resnet20(\n",
    "    in_channels: int,\n",
    "    num_classes: int,\n",
    "    conv_bias: bool = True,\n",
    "    dropout_rate: float = 0.0,\n",
    "    groups: int = 1,\n",
    "    style: Literal[\"imagenet\", \"cifar\"] = \"imagenet\",\n",
    "    activation_fn: Callable = relu,\n",
    "    normalization_layer: nn.Module = nn.BatchNorm2d,\n",
    ") -> _ResNet:\n",
    "    \"\"\"ResNet-20 model.\"\"\"\n",
    "    return _ResNet(\n",
    "        block=_BasicBlock,\n",
    "        num_blocks=[3, 3, 3],\n",
    "        in_channels=in_channels,\n",
    "        num_classes=num_classes,\n",
    "        conv_bias=conv_bias,\n",
    "        dropout_rate=dropout_rate,\n",
    "        groups=groups,\n",
    "        style=style,\n",
    "        in_planes=16,\n",
    "        activation_fn=activation_fn,\n",
    "        normalization_layer=normalization_layer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create the datasets and prepare the optimisation loop. We create two models and their corresponding optimizers. One will be trained with Cross-Entropy and the other one with label smoothing $\\alpha=0.2$. We train with label smoothing using the dedicated parameter in PyTorch's CrossEntropyLoss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "model = resnet20(in_channels=3, num_classes=10, conv_bias=False, style=\"cifar\")\n",
    "model = model.to(device)\n",
    "\n",
    "model_ls = resnet20(in_channels=3, num_classes=10, conv_bias=False, style=\"cifar\")\n",
    "model_ls = model_ls.to(device)\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.1,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4,\n",
    ")\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer,\n",
    "    milestones=[25, 50],\n",
    "    gamma=0.1,\n",
    ")\n",
    "optimizer_ls = optim.SGD(\n",
    "    model_ls.parameters(),\n",
    "    lr=0.1,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4,\n",
    ")\n",
    "scheduler_ls = optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer_ls,\n",
    "    milestones=[25, 50],\n",
    "    gamma=0.1,\n",
    ")\n",
    "\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        RandomCrop(32, padding=4),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        Normalize(\n",
    "            (0.5071, 0.4867, 0.4408),\n",
    "            (0.2675, 0.2565, 0.2761),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "test_transform = Compose(\n",
    "    [\n",
    "        ToTensor(),\n",
    "        Normalize(\n",
    "            (0.5071, 0.4867, 0.4408),\n",
    "            (0.2675, 0.2565, 0.2761),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_size = 5000\n",
    "indices = torch.load(\"cifar10_index.pth\")\n",
    "train_indices = indices[:-val_size]\n",
    "val_indices = indices[-val_size:]\n",
    "train_set = CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=train_transform,\n",
    ")\n",
    "test_set = CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=test_transform,\n",
    ")\n",
    "\n",
    "train = Subset(train_set, train_indices)\n",
    "val = copy.deepcopy(Subset(train_set, val_indices))\n",
    "val.dataset.transform = test_transform\n",
    "\n",
    "train_dl = DataLoader(train, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_dl = DataLoader(val, batch_size=128, num_workers=4, pin_memory=True)\n",
    "test_dl = DataLoader(test_set, batch_size=128, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_no_ls = nn.CrossEntropyLoss()\n",
    "criterion_ls = nn.CrossEntropyLoss(label_smoothing=0.2)\n",
    "\n",
    "if TRAIN_MODELS:\n",
    "    print(\"Training model without Label Smoothing\")\n",
    "    best_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for x, y in train_dl:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            loss = criterion_no_ls(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                total = 0\n",
    "                correct = 0\n",
    "                for x, y in val_dl:\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "                    y_hat = model(x)\n",
    "                    _, predicted = torch.max(y_hat, 1)\n",
    "                    total += y.size(0)\n",
    "                    correct += (predicted == y).sum().item()\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.2f}, Accuracy: {correct / total:.2%}\")\n",
    "                if correct / total > best_acc:\n",
    "                    best_acc = correct / total\n",
    "                    print(\"Saving best model\")\n",
    "                    torch.save(model.state_dict(), \"cifar10_resnet20.pth\")\n",
    "\n",
    "    print(\"Training model with Label Smoothing\")\n",
    "    best_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model_ls.train()\n",
    "        for x, y in train_dl:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer_ls.zero_grad()\n",
    "            y_hat = model_ls(x)\n",
    "            loss = criterion_ls(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer_ls.step()\n",
    "        scheduler_ls.step()\n",
    "        if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "            model_ls.eval()\n",
    "            with torch.no_grad():\n",
    "                total = 0\n",
    "                correct = 0\n",
    "                for x, y in val_dl:\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "                    y_hat = model_ls(x)\n",
    "                    _, predicted = torch.max(y_hat, 1)\n",
    "                    total += y.size(0)\n",
    "                    correct += (predicted == y).sum().item()\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.2f}, Accuracy: {correct / total:.2%}\")\n",
    "                if correct / total > best_acc:\n",
    "                    best_acc = correct / total\n",
    "                    print(\"Saving best model\")\n",
    "                    torch.save(model_ls.state_dict(), \"cifar10_resnet20_ls.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best models\n",
    "model.load_state_dict(torch.load(\"cifar10_resnet20.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model_ls.load_state_dict(torch.load(\"cifar10_resnet20_ls.pth\", map_location=device))\n",
    "model_ls.to(device)\n",
    "model_ls.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Risk-Coverage curves\n",
    "\n",
    "In this section, we generate the Risk-Coverage curves for the two models to compare them. We first define a function to compute the curve and apply it to the confidence scores computed for both models, one trained with Cross-Entropy and one with Label Smoothing $\\alpha=0.2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_coverage_curve(y_true, y_score, sample_weight=None, dev=\"cpu\"):\n",
    "    if sample_weight is None:\n",
    "        sample_weight = 1\n",
    "    sorted_idx = y_score.argsort(descending=True)\n",
    "    # risk for each coverage value rather than recall\n",
    "    # add one to cover situation with zero coverage, assume risk is zero\n",
    "    # when nothing is selected\n",
    "    coverage = torch.linspace(0, 1, len(y_score) + 1).to(dev)\n",
    "    # invert labels to get invalid predictions\n",
    "    sample_costs = ~(y_true.to(bool)) * sample_weight\n",
    "    sorted_cost = sample_costs[sorted_idx]\n",
    "    summed_cost = torch.cumsum(sorted_cost, 0)\n",
    "    n_selected = torch.arange(1, len(y_score) + 1).to(dev)\n",
    "    # zero risk when none selected\n",
    "    risk = torch.cat([torch.zeros(1).to(dev), summed_cost / n_selected])\n",
    "    thresholds = y_score[sorted_idx]  # select >= threshold\n",
    "    return risk, coverage, thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute the logits for both models to get the confidence scores and the correctness of the predictions. We use these tensors to compute the Risk-Coverage curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "logits = []\n",
    "scores_ls = []\n",
    "logits_ls = []\n",
    "correct_samples = []\n",
    "correct_samples_ls = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_dl:\n",
    "        x = x.to(device)\n",
    "        logit = model(x)\n",
    "        logit_ls = model_ls(x)\n",
    "        y_hat = logit.softmax(1).cpu()\n",
    "        y_hat_ls = logit_ls.softmax(1).cpu()\n",
    "        score, predicted = torch.max(y_hat, 1)\n",
    "        score_ls, predicted_ls = torch.max(y_hat_ls, 1)\n",
    "        correct = predicted == y\n",
    "        correct_ls = predicted_ls == y\n",
    "        logits.append(logit)\n",
    "        logits_ls.append(logit_ls)\n",
    "        scores.append(score)\n",
    "        scores_ls.append(score_ls)\n",
    "        correct_samples.append(correct)\n",
    "        correct_samples_ls.append(correct_ls)\n",
    "\n",
    "logits = torch.cat(logits)\n",
    "logits_ls = torch.cat(logits_ls)\n",
    "scores = torch.cat(scores)\n",
    "scores_ls = torch.cat(scores_ls)\n",
    "correct_samples = torch.cat(correct_samples)\n",
    "correct_samples_ls = torch.cat(correct_samples_ls)\n",
    "\n",
    "ce_risk, ce_cov, thresholds = risk_coverage_curve(correct_samples, scores)\n",
    "ls_risk, ls_cov, thresholds_ls = risk_coverage_curve(correct_samples_ls, scores_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "# set the style\n",
    "seaborn.set_theme()\n",
    "\n",
    "# Compute and show the risk-coverage curves\n",
    "colors = [\n",
    "    \"#1f77b4\",\n",
    "    \"#ff7f0e\",\n",
    "    \"#2ca02c\",\n",
    "    \"#d62728\",\n",
    "    \"#9467bd\",\n",
    "    \"#8c564b\",\n",
    "    \"#e377c2\",\n",
    "    \"#7f7f7f\",\n",
    "    \"#bcbd22\",\n",
    "    \"#17becf\",\n",
    "]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(\n",
    "    ce_cov * 100,\n",
    "    ce_risk * 100,\n",
    "    label=f\"CE ({ce_risk[-1]*100:.1f}, {torch.trapz(ce_risk,ce_cov).item()*100:.2f})\",\n",
    "    alpha=0.6,\n",
    "    color=\"black\",\n",
    ")\n",
    "ax[0].plot(\n",
    "    ls_cov * 100,\n",
    "    ls_risk * 100,\n",
    "    label=f\"LS $\\\\alpha=0.2$ ({ls_risk[-1]*100:.1f}, {torch.trapz(ls_risk, ls_cov).item()*100:.2f})\",\n",
    "    alpha=0.6,\n",
    "    linestyle=\"dotted\",\n",
    ")\n",
    "ax[0].set_xlabel(\"%coverage\")\n",
    "ax[0].set_ylabel(\"%risk$\\leftarrow$\")\n",
    "ax[0].legend(title=\"ResNet-20 (%error$\\downarrow$, %AURC$\\downarrow$)\\nCIFAR-10\")\n",
    "ax[0].grid(visible=True, which=\"both\")\n",
    "ax[0].set_xlim([0, 100])\n",
    "ax[0].set_ylim(ymin=0)\n",
    "ax[0].minorticks_on()\n",
    "ax[0].add_patch(patches.Rectangle((0, 0), 60, 6, fill=False, edgecolor=\"black\", linestyle=\"--\", alpha=0.5))\n",
    "# Right plot\n",
    "ax[1].plot(ce_cov * 100, ce_risk * 100, label=\"CE\", alpha=0.5, color=\"black\")\n",
    "ax[1].plot(ls_cov * 100, ls_risk * 100, label=\"LS $\\\\alpha=0.2$\", alpha=0.7, linestyle=\"dotted\")\n",
    "ax[1].grid(visible=True, which=\"both\")\n",
    "ax[1].set_xlim([0, 60])\n",
    "ax[1].set_ylim([0, 6])\n",
    "ax[1].minorticks_on()\n",
    "ax[1].set_xlabel(\"%coverage\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk-Coverage curves after logit normalization\n",
    "\n",
    "We also show the effect of logit normalization on the Risk-Coverage curves. We first define the logit normalization function and apply it to the confidence scores computed for both models. We optimize the order of the p-norm to reduce the area under the curve using validation logits. We then plot the Risk-Coverage curves for the two models and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We discard the temperature of the normalization since it had only a small effect\n",
    "def norm_logits(logits, p=2):\n",
    "    return logits / logits.norm(p=p, dim=1)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimisation of $p$ for logit normalization\n",
    "\n",
    "We now optimize the order of the p-norm to reduce the area under the curve using validation logits. We select the best value for the plots below. However, please note that the variability among the best values is very low and the results are not very sensitive to the choice of $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Precompute the logits on the validation set\n",
    "logits = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    for x, y in val_dl:\n",
    "        x = x.to(device)\n",
    "        y_hat = model(x).cpu()\n",
    "        logits.append(y_hat)\n",
    "        labels.append(y)\n",
    "\n",
    "logits = torch.cat(logits)\n",
    "labels = torch.cat(labels)\n",
    "\n",
    "ps = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "best_aurc = 1.0\n",
    "best_p = None\n",
    "best_aurcs = []\n",
    "# for plotting\n",
    "best_risk = None\n",
    "best_coverage = None\n",
    "\n",
    "correct_idx = logits.argmax(dim=-1) == labels\n",
    "for p in tqdm(ps):\n",
    "    msp_scaled = norm_logits(logits.double().cpu(), p=p).max(dim=-1).values\n",
    "    risk_scaled, cov_scaled, _ = risk_coverage_curve(\n",
    "        correct_idx,\n",
    "        msp_scaled,\n",
    "    )\n",
    "    aurc = torch.trapz(risk_scaled, cov_scaled)\n",
    "    print(f\"p={p}, AURC={aurc:.4f}\")\n",
    "    if aurc < best_aurc:\n",
    "        best_aurc = aurc\n",
    "        best_risk = risk_scaled\n",
    "        best_coverage = cov_scaled\n",
    "        best_p = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "scores_ls = []\n",
    "correct_samples = []\n",
    "correct_samples_ls = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_dl:\n",
    "        x = x.to(device)\n",
    "        # Here we remove the softmax\n",
    "        y_hat = norm_logits(model(x).cpu(), p=best_p)\n",
    "        y_hat_ls = norm_logits(model_ls(x).cpu(), p=best_p)\n",
    "        score, predicted = torch.max(y_hat, 1)\n",
    "        score_ls, predicted_ls = torch.max(y_hat_ls, 1)\n",
    "        correct = predicted == y\n",
    "        correct_ls = predicted_ls == y\n",
    "        scores.append(score)\n",
    "        scores_ls.append(score_ls)\n",
    "        correct_samples.append(correct)\n",
    "        correct_samples_ls.append(correct_ls)\n",
    "\n",
    "scores = torch.cat(scores)\n",
    "scores_ls = torch.cat(scores_ls)\n",
    "correct_samples = torch.cat(correct_samples)\n",
    "correct_samples_ls = torch.cat(correct_samples_ls)\n",
    "\n",
    "ce_risk, ce_cov, thresholds = risk_coverage_curve(correct_samples, scores)\n",
    "ls_risk, ls_cov, thresholds_ls = risk_coverage_curve(correct_samples_ls, scores_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish, we plot the Risk-Coverage curves for the two models with using normalised logits and compare them. We see that the RC curve of the model trained with LS $\\alpha=0.2$ has been much improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = [\n",
    "    \"#1f77b4\",\n",
    "    \"#ff7f0e\",\n",
    "    \"#2ca02c\",\n",
    "    \"#d62728\",\n",
    "    \"#9467bd\",\n",
    "    \"#8c564b\",\n",
    "    \"#e377c2\",\n",
    "    \"#7f7f7f\",\n",
    "    \"#bcbd22\",\n",
    "    \"#17becf\",\n",
    "]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].plot(\n",
    "    ce_cov * 100,\n",
    "    ce_risk * 100,\n",
    "    label=f\"CE ({ce_risk[-1]*100:.1f}, {torch.trapz(ce_risk,ce_cov).item()*100:.2f})\",\n",
    "    alpha=0.6,\n",
    "    color=\"black\",\n",
    ")\n",
    "ax[0].plot(\n",
    "    ls_cov * 100,\n",
    "    ls_risk * 100,\n",
    "    label=f\"LS $\\\\alpha=0.2$ ({ls_risk[-1]*100:.1f}, {torch.trapz(ls_risk, ls_cov).item()*100:.2f})\",\n",
    "    alpha=0.6,\n",
    "    linestyle=\"dotted\",\n",
    ")\n",
    "ax[0].set_xlabel(\"%coverage\")\n",
    "ax[0].set_ylabel(\"%risk$\\leftarrow$\")\n",
    "ax[0].legend(title=\"ResNet-20 (%error$\\downarrow$, %AURC$\\downarrow$)\\nCIFAR-10\")\n",
    "ax[0].grid(visible=True, which=\"both\")\n",
    "ax[0].set_xlim([0, 100])\n",
    "ax[0].set_ylim(ymin=0)\n",
    "ax[0].minorticks_on()\n",
    "ax[0].add_patch(patches.Rectangle((0, 0), 60, 6, fill=False, edgecolor=\"black\", linestyle=\"--\", alpha=0.5))\n",
    "# Right plot\n",
    "ax[1].plot(ce_cov * 100, ce_risk * 100, label=\"CE\", alpha=0.5, color=\"black\")\n",
    "ax[1].plot(ls_cov * 100, ls_risk * 100, label=\"LS $\\\\alpha=0.2$\", alpha=0.7, linestyle=\"dotted\")\n",
    "ax[1].grid(visible=True, which=\"both\")\n",
    "ax[1].set_xlim([0, 60])\n",
    "ax[1].set_ylim([0, 6])\n",
    "ax[1].minorticks_on()\n",
    "ax[1].set_xlabel(\"%coverage\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max logits given MSP plots\n",
    "\n",
    "In this part, we plot the distribution of the max logit $v_\\text{max}$ *given* the MSP $\\pi_\\text{max}$ for correct and incorrect predictions of the ResNet-20 on the CIFAR-10 evaluation subset. $v_\\text{max}$ is *lower* for for the LS model, whilst the distributions are roughly similar for CE. This empirically matches the imbalanced max logit regularisation described in the paper. We calculate the mean $\\pm$ std. in a 0.05-wide sliding window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "logits = []\n",
    "scores_ls = []\n",
    "logits_ls = []\n",
    "correct_samples = []\n",
    "correct_samples_ls = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_dl:\n",
    "        x = x.to(device)\n",
    "        logit = model(x)\n",
    "        logit_ls = model_ls(x)\n",
    "        y_hat = logit.softmax(1).cpu()\n",
    "        y_hat_ls = logit_ls.softmax(1).cpu()\n",
    "        score, predicted = torch.max(y_hat, 1)\n",
    "        score_ls, predicted_ls = torch.max(y_hat_ls, 1)\n",
    "        correct = predicted == y\n",
    "        correct_ls = predicted_ls == y\n",
    "        logits.append(logit)\n",
    "        logits_ls.append(logit_ls)\n",
    "        scores.append(score)\n",
    "        scores_ls.append(score_ls)\n",
    "        correct_samples.append(correct)\n",
    "        correct_samples_ls.append(correct_ls)\n",
    "\n",
    "logits = torch.cat(logits)\n",
    "logits_ls = torch.cat(logits_ls)\n",
    "scores = torch.cat(scores)\n",
    "scores_ls = torch.cat(scores_ls)\n",
    "correct_samples = torch.cat(correct_samples)\n",
    "correct_samples_ls = torch.cat(correct_samples_ls)\n",
    "\n",
    "ce_risk, ce_cov, thresholds = risk_coverage_curve(correct_samples, scores)\n",
    "ls_risk, ls_cov, thresholds_ls = risk_coverage_curve(correct_samples_ls, scores_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def window_average(softmax, logits, window_size=0.05, step=0.005, name=\"\") -> None:\n",
    "    \"\"\"Hue is a boolean tensor\"\"\"\n",
    "    lhat_mean, lhat_std = [], []\n",
    "    iterator = []\n",
    "    t = window_size / 2\n",
    "    end = 1 - window_size / 2\n",
    "\n",
    "    while t <= end:\n",
    "        indices = torch.where(t - window_size / 2 < softmax)[0]\n",
    "        other_indices = torch.where(softmax < t + window_size / 2)[0]\n",
    "        indices = torch.from_numpy(np.intersect1d(indices.numpy(), other_indices.numpy()))\n",
    "        if indices.size() != 0:\n",
    "            lhat_mean.append(torch.mean(logits[indices]).item())\n",
    "\n",
    "            lhat_std.append(torch.std(logits[indices]).item())\n",
    "\n",
    "        else:\n",
    "            print(\"No data at \", t)\n",
    "            lhat_mean.append(0)\n",
    "\n",
    "            lhat_std.append(0)\n",
    "\n",
    "        iterator.append(t)\n",
    "        t += step\n",
    "    return iterator, lhat_std, lhat_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msp_ce_correct = scores[correct_samples].cpu()\n",
    "msp_ls_correct = scores_ls[correct_samples_ls].cpu()\n",
    "ce_correct_max_logits = logits.max(-1).values[correct_samples].cpu()\n",
    "ls_correct_max_logits = logits_ls.max(-1).values[correct_samples_ls].cpu()\n",
    "msp_ce_err = scores[~correct_samples].cpu()\n",
    "msp_ls_err = scores_ls[~correct_samples_ls].cpu()\n",
    "ce_err_max_logits = logits.max(-1).values[~correct_samples].cpu()\n",
    "ls_err_max_logits = logits_ls.max(-1).values[~correct_samples_ls].cpu()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4), sharex=True, sharey=False)\n",
    "id_correct_color = [i / 255 for i in [79, 156, 227]]\n",
    "id_incorrect_color = [i / 255 for i in [227, 102, 79]]\n",
    "ood_color = [i / 255 for i in [164, 212, 68]]\n",
    "# CE\n",
    "unc_range = np.array([[0, 1], [3, 25]])\n",
    "x, std, y = window_average(msp_ce_correct, ce_correct_max_logits)\n",
    "axes[0].plot(x, y, label=\"Test ✓\", color=id_correct_color, alpha=0.8)\n",
    "axes[0].fill_between(\n",
    "    x,\n",
    "    np.array(y) - np.array(std),\n",
    "    np.array(y) + np.array(std),\n",
    "    alpha=0.15,\n",
    "    color=\"blue\",\n",
    ")\n",
    "x, std, y = window_average(msp_ce_err, ce_err_max_logits)\n",
    "axes[0].plot(x, y, label=\"Test ✗\", color=id_incorrect_color, alpha=0.8)\n",
    "axes[0].fill_between(x, np.array(y) - np.array(std), np.array(y) + np.array(std), alpha=0.15, color=\"red\")\n",
    "axes[0].set_title(\"CE\")\n",
    "axes[0].set_xlabel(\"$\\pi_{max}$\")\n",
    "axes[0].set_ylabel(\"$v_{max}$ | $\\pi_{max}$    (mean$\\pm$std)\")\n",
    "axes[0].set_ylim(0, 15)\n",
    "axes[0].set_xlim(0, 1)\n",
    "axes[0].legend(title=\"ResNet-20\\nCIFAR-10\")\n",
    "\n",
    "unc_range = np.array([[0, 1], [0, 15]])\n",
    "x, std, y = window_average(msp_ls_correct, ls_correct_max_logits)\n",
    "axes[1].plot(x, y, label=\"Test ✓\", color=id_correct_color, alpha=0.8)\n",
    "axes[1].fill_between(\n",
    "    x,\n",
    "    np.array(y) - np.array(std),\n",
    "    np.array(y) + np.array(std),\n",
    "    alpha=0.15,\n",
    "    color=\"blue\",\n",
    ")\n",
    "x, std, y = window_average(msp_ls_err, ls_err_max_logits)\n",
    "axes[1].plot(x, y, label=\"Test ✗\", color=id_incorrect_color, alpha=0.8)\n",
    "axes[1].fill_between(x, np.array(y) - np.array(std), np.array(y) + np.array(std), alpha=0.15, color=\"red\")\n",
    "\n",
    "\n",
    "axes[1].set_ylim(0, 6)\n",
    "\n",
    "axes[1].set_title(\"LS $\\\\alpha=0.2$\")\n",
    "\n",
    "\n",
    "axes[1].set_xlabel(\"$\\pi_{max}$\")\n",
    "axes[1].set_xlim(0, 1)\n",
    "\n",
    "\n",
    "axes[2].set_ylabel(\"relative to mean of ✓\")\n",
    "x, std, y = window_average(msp_ls_correct, ls_correct_max_logits)\n",
    "axes[2].plot(x, np.zeros(np.array(x).size), label=\"Test ✓\", color=id_correct_color, alpha=0.8)\n",
    "axes[2].fill_between(x, -np.array(std), np.array(std), alpha=0.15, color=\"blue\")\n",
    "x, std, y_ls = window_average(msp_ls_err, ls_err_max_logits)\n",
    "diff_y = np.array(y_ls) - np.array(y)\n",
    "axes[2].plot(x, diff_y, label=\"Test ✗\", color=id_incorrect_color, alpha=0.8)\n",
    "axes[2].fill_between(\n",
    "    x,\n",
    "    np.array(diff_y) - np.array(std),\n",
    "    np.array(diff_y) + np.array(std),\n",
    "    alpha=0.15,\n",
    "    color=\"red\",\n",
    ")\n",
    "\n",
    "\n",
    "axes[2].set_title(\"LS $\\\\alpha=0.2$\")\n",
    "\n",
    "\n",
    "axes[2].set_xlabel(\"$\\pi_{max}$\")\n",
    "axes[2].set_xlim(0, 1)\n",
    "axes[2].set_ylim(None, 0.3)\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".reproduce_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
